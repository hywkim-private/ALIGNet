---
#this file works as a reference for initial confihurations
#whenever a new model or dataset is initialized, its configuration file is created in accordance to this file

#configurations for settings related to cpu, gpu, parallel training
setting:
 #specify the number of gpus to use for traning
 #if nan is given, we will make use of all the possible gpus
 num_gpu: null

#configurations for initializing new datasets
#NO EFFECT IF NOT INITIALIZING A NEW DATASET
data:
 #name of the data type
 data_type: null
 #proportion of target data to split from train dataset
 target_proportion: 0.5
 #proportion of target data to split from validation dataset
 target_proportion_val: 0.5
 #the size of the voxel, sampled from pointcloud data
 grid_size: 32
 #number of train data to get from the downloaded raw data 
 train_datasize: 200
 #number of validation data to get from the downloaded raw data
 val_datasize: 30
 #the path from which to load raw data
 load_data_path: ./data/train/h5files/
 #the path to which data will be saved 
 data_path: ./data/datasets/vase/
 augment:
  #number of points to sample from mesh data
  pt_sample: 10000
 #url from which to download raw data
 #ONLY APPLIED WHEN USING DATA -D 
 url_data: https://drive.google.com/uc?export=download&id=1Vv-Jz1VpI48MOVgK3Hq6ZYrs2NDP-FQ2
  
  
#configurations for initializing new model 
#NO EFFECT IF NOT INITIALIZING A NEW MODEL 
model:
 #the path where model is saved
 model_path: ./obj/model_1/
 #index of the model to use: a variety of models are defined in model/conv
 model_idx: 2
 #the size of the grid output by the network (before upsampling is applied)
 grid_size: 9
 #the size of the voxel, sampled from pointcloud data
 image_size: 128
 #maximum number of features to learn in the network => defines complexity of the model
 maxfeat: 64
 #the weight value to apply to the L_TV_Loss
 lambda: 1.0e-4
 #setting this parameter will parameterize offset values, enabling the model to learn them 
 learn_offset: true
 #path of the data to use
 #(define relative to the uppermost directory)
 data_path: ./data/datasets/vase/
 
#configurations for training
#has effect in all modes 
train:
 #path of the model to train
 model_path: ./obj/model_1/
 #weight applied to gradient in each training step 
 step: 1.0e-5
 #size of a single batch
 batch_size: 2000
 #number of epochs to run in a single iteration
 epochs: 100
 #number of iterations to run 
 iter: 500
 #range of mask size to apply in augmenting target
 #must be smaller than vox_size
 mask_size:
  - 40
  - 60
 #how many times to augment the train target dataset 
 augment_times_tr: 300
 #how many times to augment the validation target dataset
 augment_times_val: 1
 #whether we are running checks for validation steps 
 result_check: false
 #whether we are graphing the loss (saved in result-checker object)
 graph_loss: false
 
#configurations for valid operation
#only has effect in valid mode
valid:
 #path of the path to use foir validation 
 model_path: ./obj/model_1/
 #how many times to augment the validation dataset 
 augment_times_val: 1
 #size of a single batch
 batch_size: 20
 #the range of mask size to apply to target datasets
 mask_size:
  - 10
  - 15
 #number of  samples to visualize from a batch
 num_sample: 10
